This thesis presents an extension of the Pixel-Aligned Implicit Function (PIFu) framework for 3D human reconstruction by incorporating Vision Transformers (ViT) to enhance feature extraction. The limitations of traditional convolutional neural networks (CNNs), particularly in capturing both fine-grained local details and global contextual relationships, pose significant challenges in reconstructing occluded body parts, complex geometries, and subtle variations in human poses and clothing. The proposed hybrid architecture exploits the complementary strengths of both ViT and the Hourglass network present within the PIFu framework in order to remedy these issues.

The introduction of Vision Transformers (ViT) introduces a self-attention mechanism, capable of capturing long-range dependencies and spatial relations, hence enriching the model with a larger capacity for detailed 3D reconstructions. Combining the existing pixel-aligned and detail-oriented Hourglass Network with the strong global feature extraction abilities inherent in ViT boosts the reconstruction of occluded body parts, complex geometries, and subtle variations in human poses and clothes.

Extensive evaluations were conducted using benchmark datasets such as THuman2.0 and CAPE, which provided diverse and challenging test cases.  Experimental results, including both quantitative metrics and qualitative analyses, prove that the proposed architecture outperforms existing state-of-the-art methods in terms of reconstruction accuracy and robustness.

By focusing on the integration of advanced feature extraction mechanisms into implicit representation frameworks, this thesis contributes to the ongoing development of 3D computer vision techniques, offering new possibilities for human pose estimation, high-quality 3D reconstruction, and applications in domains such as virtual reality, healthcare, and digital content creation.
